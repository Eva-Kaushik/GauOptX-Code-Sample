{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GauOptX: The Tool for Bayesian Optimization  \n",
    "\n",
    "## Reference Manual Index  \n",
    "\n",
    "=====================================================================================================  \n",
    "\n",
    "1. **What is GauOptX?**  \n",
    "\n",
    "2. **Installation and Setup**  \n",
    "\n",
    "3. **First Steps with GauOptX and Bayesian Optimization**  \n",
    "\n",
    "4. **Alternative GauOptX Interfaces: Standard, Modular, and Advanced Extensions**  \n",
    "\n",
    "5. **What Can I Do with GauOptX?**  \n",
    "    1. Bayesian optimization with arbitrary constraints.  \n",
    "    2. Parallel Bayesian optimization.  \n",
    "    3. Handling mixed variable types.  \n",
    "    4. Multi-armed bandit problems.  \n",
    "    5. Hyperparameter tuning for Scikit-learn models.  \n",
    "    6. Integrating model hyperparameter optimization.  \n",
    "    7. Input transformations and warping.  \n",
    "    8. Supporting various cost evaluation functions.  \n",
    "    9. Context-aware optimization with contextual variables.  \n",
    "    10. Integration with external objective evaluation.  \n",
    "\n",
    "6. **Currently Supported Models, Acquisitions, and Initial Designs**  \n",
    "    1. Supported initial designs.  \n",
    "    2. Implementing new models in GauOptX.  \n",
    "    3. Implementing new acquisition functions.  \n",
    "\n",
    "=====================================================================================================  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is GauOptX?  \n",
    "\n",
    "GauOptX is an advanced tool for the optimization (minimization) of black-box functions using Gaussian processes. It has been implemented in Python.  \n",
    "\n",
    "GauOptX is built upon [GPy], a powerful library for Gaussian process modeling in Python. You can explore additional notebooks demonstrating GPy's functionalities. Although primarily a tool for Bayesian optimization, GauOptX is also utilized for academic dissemination during [Gaussian Processes Summer Schools], where you can access extra labs and talks on Gaussian processes and Bayesian optimization.  \n",
    "\n",
    "The purpose of this manual is to provide a comprehensive guide to using GauOptX. The framework is [BSD-3 licensed], and we warmly welcome contributors to enhance its functionalities. If you have any questions or suggestions regarding the documentation or notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like you are drafting an installation guide for **GauOptX**. Below is a polished and structured version of your text with improved clarity, formatting, and additional details.  \n",
    "\n",
    "# **Installing GauOptX**\n",
    "\n",
    "## **1. Installing via pip (Recommended)**\n",
    "The easiest and recommended way to install **GauOptX** is using `pip`.  \n",
    "\n",
    "### **Ubuntu Users**\n",
    "Run the following commands in the terminal:  \n",
    "```bash\n",
    "sudo apt-get install python-pip  # Install pip (if not already installed)\n",
    "pip install gauoptx  # Install GauOptX\n",
    "```\n",
    "\n",
    "### **Windows & macOS Users**\n",
    "Simply run:\n",
    "```bash\n",
    "pip install gauoptx\n",
    "```\n",
    "\n",
    "## **2. Installing from Source (For Contributors)**\n",
    "If you want to modify or contribute to **GauOptX**, you should install it from source.\n",
    "\n",
    "### **Steps to Install from Source:**\n",
    "1. **Clone the GitHub Repository**  \n",
    "   ```bash\n",
    "   git clone https://github.com/your-username/gauoptx.git\n",
    "   cd gauoptx\n",
    "   ```\n",
    "\n",
    "2. **Add it to Your `$PYTHONPATH`**  \n",
    "   ```bash\n",
    "   export PYTHONPATH=$PYTHONPATH:$(pwd)\n",
    "   ```\n",
    "\n",
    "3. **Install Dependencies**  \n",
    "   Run the following to install the required dependencies:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "## **3. Dependencies**\n",
    "Several dependencies are required for **GauOptX** to function properly.\n",
    "\n",
    "✅ **Mandatory Dependencies** (Required for core functionality):  \n",
    "- `GPy`\n",
    "- `numpy`\n",
    "- `scipy`\n",
    "\n",
    "✅ **Optional Dependencies** (Needed for specific optimization features):  \n",
    "- `DIRECT` (for certain global optimization methods)\n",
    "- `cma` (Covariance Matrix Adaptation Evolution Strategy)\n",
    "- `pyDOE` (for Design of Experiments)\n",
    "\n",
    "To install all dependencies, run:\n",
    "```bash\n",
    "pip install GPy numpy scipy DIRECT cma pyDOE\n",
    "```\n",
    "\n",
    "## **4. Verifying Installation**\n",
    "To check that **GauOptX** has been installed successfully, run:\n",
    "```python\n",
    "import gauoptx\n",
    "print(gauoptx.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Alternative GauOptX interfaces: Standard, Modular and Spearmint\n",
    "\n",
    "GauOptX has different interfaces oriented to different types of users. Apart from the general interface (detailed in the introductory manual) you can use GauOptX in a modular way: you can implement and use your some elements of the optimization process, such us a new model or acquisition function, but still use the main backbone of the package. \n",
    "\n",
    "Also, we have developed and GauOptX interface with Spearmint but this only covers some general features that are available in GauOptX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What can I do with GauOptX?\n",
    "\n",
    "There are several options implemented in GauOptX that allows to cover a wide range of specific optimization problems. We have implemented a collection of notebooks to explain these functionalities separately but they can be easily combined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Bayesian optimization with arbitrary restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With GauOptX you can solve optimization problems with arbitrary non trivial restrictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Parallel Bayesian optimization\n",
    "The main bottleneck when using Bayesian optimization is the cost of evaluating the objective function. In the notebook [GauOptX: parallel Bayesian Optimization] you can learn more about the different parallel methods currently implemented in GauOptX.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Mixing different types of variables\n",
    "In GauOptX you can easily combine different types of variables in the optimization. Currently you can use discrete an continuous variables. The way GauOptX handles discrete variables is by marginally optimizing the acquisition functions over combinations of feasible values. This may slow down the optimization if many discrete variables are used but it avoids rounding errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Armed bandits problems\n",
    "\n",
    "Armed bandits optimization problems are a particular case of Bayesian Optimization that appear when the domain of the function to optimize is entirely discrete. This has several advantages with respect to optimize in continuous domains. The most remarkable is that the optimization of the acquisition function can be done by taking the $arg min$ of all candidate points while the rest of the BO theory applies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Tuning scikit-learn models\n",
    "\n",
    "[Scikit-learn](http://scikit-learn.org/stable/) is a very popular library with a large variety of useful methods in Machine Learning. Have a look to the notebook [GauOptX: configuring Scikit-learn methods] to learn how learn the parameters of Scikit-learn methods using GauOptX. You will learn how to automatically tune the parameters of a Support Vector Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Integrating the model hyper parameters\n",
    "Maximum Likelihood estimation can be a very instable choice to tune the surrogate model hyper parameters, especially in the fist steps of the optimization. When using a GP model as a surrogate of your function you can integrate the most common acquisition functions with respect to the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Input warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Using various cost evaluation functions\n",
    "The cost of evaluating the objective can be a crucial factor in the optimization process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Contextual variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the optimization phase, you may want to fix the value of some of the variables. These variables are called context as they are part of the objective but are fixed when the aquisition is optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 External objective evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot define your objective function in Python, you have an option of evaluating it externally, and calling GauOptX to suggest the next locations to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Currently supported models and acquisitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, you can initialize your model with three types of initial designs: \n",
    "- Random, \n",
    "- Latin Hypercubes and \n",
    "- Sobol sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implemeting new models\n",
    "\n",
    "The currently available models in GauOptX are:\n",
    "\n",
    "- Standard GPs (with MLE and HMC inference)\n",
    "- Sparse GPs\n",
    "- Warperd GPs (both for the input and the output)\n",
    "- Random Forrests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Implementing new acquisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The currently available acquisition functions in GauOptX are:\n",
    "\n",
    "- Expected Improvement.\n",
    "- Maximum Probability of Improvement.\n",
    "- Lower Confidence Bound.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
